{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb667027",
   "metadata": {},
   "source": [
    "#### Group Information\n",
    "\n",
    "Group No: \n",
    "\n",
    "- Member 1: Angeline Teoh Qee (159023)\n",
    "- Member 2: LeeJu Yi (158713)\n",
    "- Member 3: Lee Ter Qin (159389)\n",
    "- Member 4: Jee Rou Yi (159273)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8f1a6",
   "metadata": {},
   "source": [
    "### Project Description\n",
    "The dataset used in this project is dermatology. The differential diagnosis of erythemato-squamous diseases is a real problem in dermatology. They all share the clinical features of erythema and scaling, with very little differences. The predictive models that we have chosen to perform multiclass classification are **K-Nearest Neighbors (KNN)** and **Neural Network**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc1577",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac4a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf223c5",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e45fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"C:/Users/Angeline/OneDrive/Desktop/Year2/Y2S2/CPC251/Project/Dermatology/dermatology.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e97df",
   "metadata": {},
   "source": [
    "Next, to understand the dataset, we will be printing information about the dataframe, such as number of columns, column labels, column data types, memory usage, range index, and the number of cells in each column (non-null values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2dbe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 365 entries, 0 to 364\n",
      "Data columns (total 35 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   2       365 non-null    int64 \n",
      " 1   2.1     365 non-null    int64 \n",
      " 2   0       365 non-null    int64 \n",
      " 3   3       365 non-null    int64 \n",
      " 4   0.1     365 non-null    int64 \n",
      " 5   0.2     365 non-null    int64 \n",
      " 6   0.3     365 non-null    int64 \n",
      " 7   0.4     365 non-null    int64 \n",
      " 8   1       365 non-null    int64 \n",
      " 9   0.5     365 non-null    int64 \n",
      " 10  0.6     365 non-null    int64 \n",
      " 11  0.7     365 non-null    int64 \n",
      " 12  0.8     365 non-null    int64 \n",
      " 13  0.9     365 non-null    int64 \n",
      " 14  0.10    365 non-null    int64 \n",
      " 15  3.1     365 non-null    int64 \n",
      " 16  2.2     365 non-null    int64 \n",
      " 17  0.11    365 non-null    int64 \n",
      " 18  0.12    365 non-null    int64 \n",
      " 19  0.13    365 non-null    int64 \n",
      " 20  0.14    365 non-null    int64 \n",
      " 21  0.15    365 non-null    int64 \n",
      " 22  0.16    365 non-null    int64 \n",
      " 23  0.17    365 non-null    int64 \n",
      " 24  0.18    365 non-null    int64 \n",
      " 25  0.19    365 non-null    int64 \n",
      " 26  0.20    365 non-null    int64 \n",
      " 27  3.2     365 non-null    int64 \n",
      " 28  0.21    365 non-null    int64 \n",
      " 29  0.22    365 non-null    int64 \n",
      " 30  0.23    365 non-null    int64 \n",
      " 31  1.1     365 non-null    int64 \n",
      " 32  0.24    365 non-null    int64 \n",
      " 33  55      365 non-null    object\n",
      " 34  2.3     365 non-null    int64 \n",
      "dtypes: int64(34), object(1)\n",
      "memory usage: 99.9+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53bf163",
   "metadata": {},
   "source": [
    "Since there are no label for each column, so we add column name for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8804bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name=['erythema', 'scaling', 'definite_borders', 'itching',\n",
    "       'koebner_phenomenon', 'polygonal_papules', 'follicular_papules',\n",
    "       'oral_mucosal_involvement', 'knee_and_elbow_involvement',\n",
    "       'scalp_involvement', 'family_history', 'melanin_incontinence',\n",
    "       'eosinophils_in_the_infiltrate', 'pnl_infiltrate',\n",
    "       'fibrosis_of_the_papillary_dermis', 'exocytosis', 'acanthosis',\n",
    "       'hyperkeratosis', 'parakeratosis', 'clubbing_of_the_rete_ridges',\n",
    "       'elongation_of_the_rete_ridges',\n",
    "       'thinning_of_the_suprapapillary_epidermis', 'spongiform_pustule',\n",
    "       'munro_microabcess', 'focal_hypergranulosis',\n",
    "       'disappearance_of_the_granular_layer',\n",
    "       'vacuolisation_and_damage_of_basal_layer', 'spongiosis',\n",
    "       'saw-tooth_appearance_of_retes', 'follicular_horn_plug',\n",
    "       'perifollicular_parakeratosis', 'inflammatory_monoluclear_inflitrate',\n",
    "       'band-like_infiltrate', 'age', 'class']\n",
    "dataset.columns=col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410ba7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>erythema</th>\n",
       "      <th>scaling</th>\n",
       "      <th>definite_borders</th>\n",
       "      <th>itching</th>\n",
       "      <th>koebner_phenomenon</th>\n",
       "      <th>polygonal_papules</th>\n",
       "      <th>follicular_papules</th>\n",
       "      <th>oral_mucosal_involvement</th>\n",
       "      <th>knee_and_elbow_involvement</th>\n",
       "      <th>scalp_involvement</th>\n",
       "      <th>...</th>\n",
       "      <th>disappearance_of_the_granular_layer</th>\n",
       "      <th>vacuolisation_and_damage_of_basal_layer</th>\n",
       "      <th>spongiosis</th>\n",
       "      <th>saw-tooth_appearance_of_retes</th>\n",
       "      <th>follicular_horn_plug</th>\n",
       "      <th>perifollicular_parakeratosis</th>\n",
       "      <th>inflammatory_monoluclear_inflitrate</th>\n",
       "      <th>band-like_infiltrate</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   erythema  scaling  definite_borders  itching  koebner_phenomenon  \\\n",
       "0         3        3                 3        2                   1   \n",
       "1         2        1                 2        3                   1   \n",
       "2         2        2                 2        0                   0   \n",
       "3         2        3                 2        2                   2   \n",
       "4         2        3                 2        0                   0   \n",
       "\n",
       "   polygonal_papules  follicular_papules  oral_mucosal_involvement  \\\n",
       "0                  0                   0                         0   \n",
       "1                  3                   0                         3   \n",
       "2                  0                   0                         0   \n",
       "3                  2                   0                         2   \n",
       "4                  0                   0                         0   \n",
       "\n",
       "   knee_and_elbow_involvement  scalp_involvement  ...  \\\n",
       "0                           1                  1  ...   \n",
       "1                           0                  0  ...   \n",
       "2                           3                  2  ...   \n",
       "3                           0                  0  ...   \n",
       "4                           0                  0  ...   \n",
       "\n",
       "   disappearance_of_the_granular_layer  \\\n",
       "0                                    0   \n",
       "1                                    0   \n",
       "2                                    3   \n",
       "3                                    2   \n",
       "4                                    0   \n",
       "\n",
       "   vacuolisation_and_damage_of_basal_layer  spongiosis  \\\n",
       "0                                        0           0   \n",
       "1                                        2           3   \n",
       "2                                        0           0   \n",
       "3                                        3           2   \n",
       "4                                        0           2   \n",
       "\n",
       "   saw-tooth_appearance_of_retes  follicular_horn_plug  \\\n",
       "0                              0                     0   \n",
       "1                              2                     0   \n",
       "2                              0                     0   \n",
       "3                              3                     0   \n",
       "4                              0                     0   \n",
       "\n",
       "   perifollicular_parakeratosis  inflammatory_monoluclear_inflitrate  \\\n",
       "0                             0                                    1   \n",
       "1                             0                                    2   \n",
       "2                             0                                    3   \n",
       "3                             0                                    2   \n",
       "4                             0                                    1   \n",
       "\n",
       "   band-like_infiltrate  age  class  \n",
       "0                     0    8      1  \n",
       "1                     3   26      3  \n",
       "2                     0   40      1  \n",
       "3                     3   45      3  \n",
       "4                     0   41      2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453d737",
   "metadata": {},
   "source": [
    "#### Split the dataset\n",
    "Split the dataset into training, validation and test sets.\n",
    "\n",
    "First of all, we will get an insight of the information of the dataset with column name added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8064bc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 365 entries, 0 to 364\n",
      "Data columns (total 35 columns):\n",
      " #   Column                                    Non-Null Count  Dtype \n",
      "---  ------                                    --------------  ----- \n",
      " 0   erythema                                  365 non-null    int64 \n",
      " 1   scaling                                   365 non-null    int64 \n",
      " 2   definite_borders                          365 non-null    int64 \n",
      " 3   itching                                   365 non-null    int64 \n",
      " 4   koebner_phenomenon                        365 non-null    int64 \n",
      " 5   polygonal_papules                         365 non-null    int64 \n",
      " 6   follicular_papules                        365 non-null    int64 \n",
      " 7   oral_mucosal_involvement                  365 non-null    int64 \n",
      " 8   knee_and_elbow_involvement                365 non-null    int64 \n",
      " 9   scalp_involvement                         365 non-null    int64 \n",
      " 10  family_history                            365 non-null    int64 \n",
      " 11  melanin_incontinence                      365 non-null    int64 \n",
      " 12  eosinophils_in_the_infiltrate             365 non-null    int64 \n",
      " 13  pnl_infiltrate                            365 non-null    int64 \n",
      " 14  fibrosis_of_the_papillary_dermis          365 non-null    int64 \n",
      " 15  exocytosis                                365 non-null    int64 \n",
      " 16  acanthosis                                365 non-null    int64 \n",
      " 17  hyperkeratosis                            365 non-null    int64 \n",
      " 18  parakeratosis                             365 non-null    int64 \n",
      " 19  clubbing_of_the_rete_ridges               365 non-null    int64 \n",
      " 20  elongation_of_the_rete_ridges             365 non-null    int64 \n",
      " 21  thinning_of_the_suprapapillary_epidermis  365 non-null    int64 \n",
      " 22  spongiform_pustule                        365 non-null    int64 \n",
      " 23  munro_microabcess                         365 non-null    int64 \n",
      " 24  focal_hypergranulosis                     365 non-null    int64 \n",
      " 25  disappearance_of_the_granular_layer       365 non-null    int64 \n",
      " 26  vacuolisation_and_damage_of_basal_layer   365 non-null    int64 \n",
      " 27  spongiosis                                365 non-null    int64 \n",
      " 28  saw-tooth_appearance_of_retes             365 non-null    int64 \n",
      " 29  follicular_horn_plug                      365 non-null    int64 \n",
      " 30  perifollicular_parakeratosis              365 non-null    int64 \n",
      " 31  inflammatory_monoluclear_inflitrate       365 non-null    int64 \n",
      " 32  band-like_infiltrate                      365 non-null    int64 \n",
      " 33  age                                       365 non-null    object\n",
      " 34  class                                     365 non-null    int64 \n",
      "dtypes: int64(34), object(1)\n",
      "memory usage: 99.9+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c2c69e",
   "metadata": {},
   "source": [
    "Drop the rows that contains unknown value from the dataset.\n",
    "\n",
    "In this case, we have some \"?\" in the column \"age\".\n",
    "\n",
    "Hence, we drop the respective rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6125a76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>erythema</th>\n",
       "      <th>scaling</th>\n",
       "      <th>definite_borders</th>\n",
       "      <th>itching</th>\n",
       "      <th>koebner_phenomenon</th>\n",
       "      <th>polygonal_papules</th>\n",
       "      <th>follicular_papules</th>\n",
       "      <th>oral_mucosal_involvement</th>\n",
       "      <th>knee_and_elbow_involvement</th>\n",
       "      <th>scalp_involvement</th>\n",
       "      <th>...</th>\n",
       "      <th>disappearance_of_the_granular_layer</th>\n",
       "      <th>vacuolisation_and_damage_of_basal_layer</th>\n",
       "      <th>spongiosis</th>\n",
       "      <th>saw-tooth_appearance_of_retes</th>\n",
       "      <th>follicular_horn_plug</th>\n",
       "      <th>perifollicular_parakeratosis</th>\n",
       "      <th>inflammatory_monoluclear_inflitrate</th>\n",
       "      <th>band-like_infiltrate</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     erythema  scaling  definite_borders  itching  koebner_phenomenon  \\\n",
       "32          2        2                 1        0                   0   \n",
       "33          2        1                 0        0                   2   \n",
       "34          2        2                 1        2                   0   \n",
       "35          2        1                 2        3                   2   \n",
       "261         2        1                 0        2                   0   \n",
       "262         1        1                 1        3                   0   \n",
       "263         1        1                 0        2                   0   \n",
       "264         1        1                 0        3                   0   \n",
       "\n",
       "     polygonal_papules  follicular_papules  oral_mucosal_involvement  \\\n",
       "32                   0                   0                         0   \n",
       "33                   0                   0                         0   \n",
       "34                   0                   0                         0   \n",
       "35                   3                   0                         2   \n",
       "261                  0                   0                         0   \n",
       "262                  0                   0                         0   \n",
       "263                  0                   0                         0   \n",
       "264                  0                   0                         0   \n",
       "\n",
       "     knee_and_elbow_involvement  scalp_involvement  ...  \\\n",
       "32                            1                  0  ...   \n",
       "33                            0                  0  ...   \n",
       "34                            0                  0  ...   \n",
       "35                            0                  0  ...   \n",
       "261                           0                  0  ...   \n",
       "262                           0                  0  ...   \n",
       "263                           1                  0  ...   \n",
       "264                           0                  0  ...   \n",
       "\n",
       "     disappearance_of_the_granular_layer  \\\n",
       "32                                     0   \n",
       "33                                     0   \n",
       "34                                     0   \n",
       "35                                     0   \n",
       "261                                    0   \n",
       "262                                    0   \n",
       "263                                    0   \n",
       "264                                    0   \n",
       "\n",
       "     vacuolisation_and_damage_of_basal_layer  spongiosis  \\\n",
       "32                                         0           0   \n",
       "33                                         0           0   \n",
       "34                                         0           0   \n",
       "35                                         2           0   \n",
       "261                                        0           2   \n",
       "262                                        0           3   \n",
       "263                                        0           2   \n",
       "264                                        0           2   \n",
       "\n",
       "     saw-tooth_appearance_of_retes  follicular_horn_plug  \\\n",
       "32                               0                     0   \n",
       "33                               0                     0   \n",
       "34                               0                     0   \n",
       "35                               2                     0   \n",
       "261                              0                     0   \n",
       "262                              0                     0   \n",
       "263                              0                     0   \n",
       "264                              0                     0   \n",
       "\n",
       "     perifollicular_parakeratosis  inflammatory_monoluclear_inflitrate  \\\n",
       "32                              0                                    0   \n",
       "33                              0                                    0   \n",
       "34                              0                                    0   \n",
       "35                              0                                    0   \n",
       "261                             0                                    3   \n",
       "262                             0                                    2   \n",
       "263                             0                                    3   \n",
       "264                             0                                    3   \n",
       "\n",
       "     band-like_infiltrate  age  class  \n",
       "32                      0    ?      1  \n",
       "33                      0    ?      4  \n",
       "34                      0    ?      2  \n",
       "35                      3    ?      3  \n",
       "261                     0    ?      5  \n",
       "262                     0    ?      5  \n",
       "263                     0    ?      5  \n",
       "264                     0    ?      5  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[dataset['age'] == '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ec1e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(dataset.loc[dataset['age']=='?'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dcad44",
   "metadata": {},
   "source": [
    "Check again if there any unknown data in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f963050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>erythema</th>\n",
       "      <th>scaling</th>\n",
       "      <th>definite_borders</th>\n",
       "      <th>itching</th>\n",
       "      <th>koebner_phenomenon</th>\n",
       "      <th>polygonal_papules</th>\n",
       "      <th>follicular_papules</th>\n",
       "      <th>oral_mucosal_involvement</th>\n",
       "      <th>knee_and_elbow_involvement</th>\n",
       "      <th>scalp_involvement</th>\n",
       "      <th>...</th>\n",
       "      <th>disappearance_of_the_granular_layer</th>\n",
       "      <th>vacuolisation_and_damage_of_basal_layer</th>\n",
       "      <th>spongiosis</th>\n",
       "      <th>saw-tooth_appearance_of_retes</th>\n",
       "      <th>follicular_horn_plug</th>\n",
       "      <th>perifollicular_parakeratosis</th>\n",
       "      <th>inflammatory_monoluclear_inflitrate</th>\n",
       "      <th>band-like_infiltrate</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [erythema, scaling, definite_borders, itching, koebner_phenomenon, polygonal_papules, follicular_papules, oral_mucosal_involvement, knee_and_elbow_involvement, scalp_involvement, family_history, melanin_incontinence, eosinophils_in_the_infiltrate, pnl_infiltrate, fibrosis_of_the_papillary_dermis, exocytosis, acanthosis, hyperkeratosis, parakeratosis, clubbing_of_the_rete_ridges, elongation_of_the_rete_ridges, thinning_of_the_suprapapillary_epidermis, spongiform_pustule, munro_microabcess, focal_hypergranulosis, disappearance_of_the_granular_layer, vacuolisation_and_damage_of_basal_layer, spongiosis, saw-tooth_appearance_of_retes, follicular_horn_plug, perifollicular_parakeratosis, inflammatory_monoluclear_inflitrate, band-like_infiltrate, age, class]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 35 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[dataset['age'] == '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25fca5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>erythema</th>\n",
       "      <th>scaling</th>\n",
       "      <th>definite_borders</th>\n",
       "      <th>itching</th>\n",
       "      <th>koebner_phenomenon</th>\n",
       "      <th>polygonal_papules</th>\n",
       "      <th>follicular_papules</th>\n",
       "      <th>oral_mucosal_involvement</th>\n",
       "      <th>knee_and_elbow_involvement</th>\n",
       "      <th>scalp_involvement</th>\n",
       "      <th>...</th>\n",
       "      <th>disappearance_of_the_granular_layer</th>\n",
       "      <th>vacuolisation_and_damage_of_basal_layer</th>\n",
       "      <th>spongiosis</th>\n",
       "      <th>saw-tooth_appearance_of_retes</th>\n",
       "      <th>follicular_horn_plug</th>\n",
       "      <th>perifollicular_parakeratosis</th>\n",
       "      <th>inflammatory_monoluclear_inflitrate</th>\n",
       "      <th>band-like_infiltrate</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     erythema  scaling  definite_borders  itching  koebner_phenomenon  \\\n",
       "0           3        3                 3        2                   1   \n",
       "1           2        1                 2        3                   1   \n",
       "2           2        2                 2        0                   0   \n",
       "3           2        3                 2        2                   2   \n",
       "4           2        3                 2        0                   0   \n",
       "..        ...      ...               ...      ...                 ...   \n",
       "360         2        1                 1        0                   1   \n",
       "361         3        2                 1        0                   1   \n",
       "362         3        2                 2        2                   3   \n",
       "363         2        1                 3        1                   2   \n",
       "364         3        2                 2        0                   0   \n",
       "\n",
       "     polygonal_papules  follicular_papules  oral_mucosal_involvement  \\\n",
       "0                    0                   0                         0   \n",
       "1                    3                   0                         3   \n",
       "2                    0                   0                         0   \n",
       "3                    2                   0                         2   \n",
       "4                    0                   0                         0   \n",
       "..                 ...                 ...                       ...   \n",
       "360                  0                   0                         0   \n",
       "361                  0                   0                         0   \n",
       "362                  2                   0                         2   \n",
       "363                  3                   0                         2   \n",
       "364                  0                   0                         0   \n",
       "\n",
       "     knee_and_elbow_involvement  scalp_involvement  ...  \\\n",
       "0                             1                  1  ...   \n",
       "1                             0                  0  ...   \n",
       "2                             3                  2  ...   \n",
       "3                             0                  0  ...   \n",
       "4                             0                  0  ...   \n",
       "..                          ...                ...  ...   \n",
       "360                           0                  0  ...   \n",
       "361                           0                  0  ...   \n",
       "362                           0                  0  ...   \n",
       "363                           0                  0  ...   \n",
       "364                           3                  3  ...   \n",
       "\n",
       "     disappearance_of_the_granular_layer  \\\n",
       "0                                      0   \n",
       "1                                      0   \n",
       "2                                      3   \n",
       "3                                      2   \n",
       "4                                      0   \n",
       "..                                   ...   \n",
       "360                                    0   \n",
       "361                                    1   \n",
       "362                                    0   \n",
       "363                                    0   \n",
       "364                                    2   \n",
       "\n",
       "     vacuolisation_and_damage_of_basal_layer  spongiosis  \\\n",
       "0                                          0           0   \n",
       "1                                          2           3   \n",
       "2                                          0           0   \n",
       "3                                          3           2   \n",
       "4                                          0           2   \n",
       "..                                       ...         ...   \n",
       "360                                        0           1   \n",
       "361                                        0           1   \n",
       "362                                        3           0   \n",
       "363                                        2           0   \n",
       "364                                        0           0   \n",
       "\n",
       "     saw-tooth_appearance_of_retes  follicular_horn_plug  \\\n",
       "0                                0                     0   \n",
       "1                                2                     0   \n",
       "2                                0                     0   \n",
       "3                                3                     0   \n",
       "4                                0                     0   \n",
       "..                             ...                   ...   \n",
       "360                              0                     0   \n",
       "361                              0                     0   \n",
       "362                              3                     0   \n",
       "363                              1                     0   \n",
       "364                              0                     0   \n",
       "\n",
       "     perifollicular_parakeratosis  inflammatory_monoluclear_inflitrate  \\\n",
       "0                               0                                    1   \n",
       "1                               0                                    2   \n",
       "2                               0                                    3   \n",
       "3                               0                                    2   \n",
       "4                               0                                    1   \n",
       "..                            ...                                  ...   \n",
       "360                             0                                    2   \n",
       "361                             0                                    2   \n",
       "362                             0                                    2   \n",
       "363                             0                                    2   \n",
       "364                             0                                    3   \n",
       "\n",
       "     band-like_infiltrate  age  class  \n",
       "0                       0    8      1  \n",
       "1                       3   26      3  \n",
       "2                       0   40      1  \n",
       "3                       3   45      3  \n",
       "4                       0   41      2  \n",
       "..                    ...  ...    ...  \n",
       "360                     0   25      4  \n",
       "361                     0   36      4  \n",
       "362                     3   28      3  \n",
       "363                     3   50      3  \n",
       "364                     0   35      1  \n",
       "\n",
       "[357 rows x 35 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f54471",
   "metadata": {},
   "source": [
    "After dropping the rows with unknown age, we have 357 rows left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531071d2",
   "metadata": {},
   "source": [
    "#### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e37247b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns = ['class']).copy() #features\n",
    "y = dataset['class'] #target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7dd86",
   "metadata": {},
   "source": [
    "In the first step we will split the data in training and remaining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01e0d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b0ef5",
   "metadata": {},
   "source": [
    "Now since we want the valid and test size to be equal (15% each of overall data). \n",
    "We have to define valid_size=0.5 (that is 50% of remaining data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62e367a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 34)\n",
      "(249,)\n",
      "(54, 34)\n",
      "(54,)\n",
      "(54, 34)\n",
      "(54,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "\n",
    "print(X_train.shape), print(y_train.shape)\n",
    "print(X_valid.shape), print(y_valid.shape)\n",
    "print(X_test.shape), print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1599896",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeec7b1",
   "metadata": {},
   "source": [
    "Since KNN works based on the distance between data points, it’s important that we standardize the data before training the model. We need to standardize the data first by using StandardScale. The standardized dataset will be saved in the variables \"X_train_transform\", \"X_valid_transform\" and \"X_test_transform\".\n",
    "\n",
    "The purpose of standardize data for KNN is to gives all features the same influence on the distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58a57dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_transform = scaler.fit_transform(X_train)\n",
    "X_valid_transform = scaler.fit_transform(X_valid)\n",
    "X_test_transform = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0485fbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.        , 0.33333333, ..., 1.        , 0.        ,\n",
       "        0.82352941],\n",
       "       [1.        , 0.66666667, 0.33333333, ..., 0.33333333, 0.        ,\n",
       "        0.63235294],\n",
       "       [0.33333333, 0.33333333, 0.66666667, ..., 0.66666667, 1.        ,\n",
       "        0.63235294],\n",
       "       ...,\n",
       "       [0.66666667, 0.33333333, 0.66666667, ..., 0.33333333, 0.        ,\n",
       "        0.14705882],\n",
       "       [1.        , 0.66666667, 0.66666667, ..., 0.66666667, 0.        ,\n",
       "        0.04411765],\n",
       "       [0.66666667, 0.66666667, 1.        , ..., 1.        , 1.        ,\n",
       "        0.29411765]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "292d1cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.33333333, 0.        , ..., 0.33333333, 0.        ,\n",
       "        1.        ],\n",
       "       [0.5       , 0.66666667, 0.66666667, ..., 0.66666667, 0.        ,\n",
       "        0.59677419],\n",
       "       [1.        , 0.66666667, 0.66666667, ..., 0.66666667, 0.        ,\n",
       "        0.83870968],\n",
       "       ...,\n",
       "       [0.5       , 0.66666667, 0.66666667, ..., 1.        , 0.        ,\n",
       "        0.51612903],\n",
       "       [1.        , 0.66666667, 0.33333333, ..., 0.66666667, 0.        ,\n",
       "        0.27419355],\n",
       "       [0.5       , 0.33333333, 0.66666667, ..., 0.66666667, 1.        ,\n",
       "        0.69354839]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5472e48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.66666667, 0.66666667, ..., 0.66666667, 0.        ,\n",
       "        0.88571429],\n",
       "       [0.33333333, 0.33333333, 0.33333333, ..., 0.        , 0.66666667,\n",
       "        0.        ],\n",
       "       [1.        , 0.33333333, 0.66666667, ..., 0.66666667, 0.        ,\n",
       "        0.24285714],\n",
       "       ...,\n",
       "       [0.66666667, 0.33333333, 0.66666667, ..., 1.        , 0.        ,\n",
       "        0.88571429],\n",
       "       [0.33333333, 1.        , 0.33333333, ..., 1.        , 0.        ,\n",
       "        0.85714286],\n",
       "       [0.66666667, 1.        , 0.66666667, ..., 0.66666667, 0.        ,\n",
       "        0.47142857]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b071e7",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "Perform feature selection to select the relevant features. <br>\n",
    "\n",
    "*For KNN:*\n",
    "- We will perform feature selection using forward selection strategy and selects the best subset of features based on accuracy.\n",
    "\n",
    "\n",
    "*For Neural Network:*\n",
    "- Feature selection is not as important for neural networks as it is for other algorithms. Neural Networks are “smarter” algorithms, they have internal weights that adjust to minimize a cost function. Less important features will be attributed comparatively lower importance with respect to highly predictive variables1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a12f037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\angeline\\anaconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from mlxtend) (1.20.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from mlxtend) (1.2.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from mlxtend) (3.4.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from mlxtend) (58.0.4)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from mlxtend) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from mlxtend) (1.7.3)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from mlxtend) (1.3.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (8.4.0)\n",
      "Requirement already satisfied: six in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\angeline\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend\n",
    "import joblib\n",
    "import sys\n",
    "sys.modules['sklearn.externals.joblib'] = joblib\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71fcfad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc=SVC()\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "forward_fs_best=sfs(estimator = svc, k_features = 'best', forward = True, scoring = 'accuracy')\n",
    "sfs_forward_best=forward_fs_best.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3144a",
   "metadata": {},
   "source": [
    "The variable 'feature_index' retrieves the indices of the selected features after performing the forward feature selection using the SequentialFeatureSelector object (sfs_forward_best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "550e11ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('koebner_phenomenon',\n",
       " 'follicular_papules',\n",
       " 'fibrosis_of_the_papillary_dermis',\n",
       " 'clubbing_of_the_rete_ridges',\n",
       " 'disappearance_of_the_granular_layer',\n",
       " 'saw-tooth_appearance_of_retes',\n",
       " 'perifollicular_parakeratosis')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_index = np.asarray(sfs_forward_best.k_feature_idx_)\n",
    "sfs_forward_best.k_feature_names_ #Display the selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ecf37d",
   "metadata": {},
   "source": [
    "In forward selection, the first variable selected for an entry into the constructed model is the one with the largest correlation with the dependent variable. In this case, 'scaling' has the largest correlation with the target class. Other features like 'age' and 'itching' are not informative in determining the class of disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0ffd00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To convert from nparray into dataframe:\n",
    "X_train_transform_f = pd.DataFrame(X_train_transform)\n",
    "X_valid_transform_f = pd.DataFrame(X_valid_transform)\n",
    "X_test_transform_f = pd.DataFrame(X_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dddfc493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>14</th>\n",
       "      <th>19</th>\n",
       "      <th>25</th>\n",
       "      <th>28</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           4    6         14        19        25        28        30\n",
       "0    0.000000  0.0  0.666667  0.000000  0.000000  0.000000  0.000000\n",
       "1    0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "2    0.666667  0.0  0.000000  0.000000  0.000000  1.000000  0.000000\n",
       "3    0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "4    0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.333333\n",
       "..        ...  ...       ...       ...       ...       ...       ...\n",
       "244  0.333333  0.0  0.000000  0.333333  0.666667  0.000000  0.000000\n",
       "245  0.666667  0.0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "246  0.333333  0.0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "247  0.000000  1.0  0.000000  0.000000  0.000000  0.000000  0.666667\n",
       "248  0.333333  0.0  0.000000  0.000000  0.000000  0.666667  0.000000\n",
       "\n",
       "[249 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_fs = X_train_transform_f.iloc[:,feature_index]\n",
    "X_train_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c912afe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>14</th>\n",
       "      <th>19</th>\n",
       "      <th>25</th>\n",
       "      <th>28</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          4    6         14        19        25        28   30\n",
       "0   0.000000  0.0  1.000000  0.000000  0.000000  0.000000  0.0\n",
       "1   0.333333  0.0  0.000000  1.000000  0.333333  0.000000  0.0\n",
       "2   0.666667  0.0  0.000000  0.666667  0.000000  0.000000  0.0\n",
       "3   0.000000  1.0  0.000000  0.000000  0.000000  0.000000  1.0\n",
       "4   0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "5   0.000000  0.0  0.000000  0.000000  0.000000  0.333333  0.0\n",
       "6   0.333333  0.0  0.000000  0.666667  0.000000  0.000000  0.0\n",
       "7   0.000000  0.0  0.333333  0.000000  0.000000  0.000000  0.0\n",
       "8   0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "9   0.000000  0.0  0.000000  0.333333  0.000000  0.000000  0.0\n",
       "10  1.000000  0.0  0.000000  0.000000  0.000000  0.666667  0.0\n",
       "11  0.000000  0.0  1.000000  0.000000  0.000000  0.000000  0.0\n",
       "12  0.000000  0.0  0.000000  0.000000  0.000000  0.666667  0.0\n",
       "13  0.333333  0.0  0.000000  0.666667  0.000000  0.000000  0.0\n",
       "14  0.666667  0.0  0.000000  0.000000  0.000000  0.666667  0.0\n",
       "15  0.333333  0.5  0.000000  0.666667  0.000000  0.000000  0.0\n",
       "16  0.000000  0.0  0.000000  0.000000  0.000000  1.000000  0.0\n",
       "17  0.333333  0.0  0.000000  0.000000  0.000000  1.000000  0.0\n",
       "18  0.333333  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "19  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "20  0.333333  0.0  0.000000  0.000000  0.000000  0.666667  0.0\n",
       "21  0.333333  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "22  0.666667  0.0  0.000000  0.000000  0.333333  0.000000  0.0\n",
       "23  0.000000  0.0  0.000000  0.000000  0.000000  1.000000  0.0\n",
       "24  0.000000  1.0  1.000000  0.000000  0.000000  0.000000  0.0\n",
       "25  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "26  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "27  0.000000  0.0  0.000000  0.666667  0.333333  0.000000  0.0\n",
       "28  0.000000  0.0  0.000000  1.000000  0.666667  0.000000  0.0\n",
       "29  0.666667  0.0  0.000000  0.000000  0.000000  0.666667  0.0\n",
       "30  0.333333  0.0  0.000000  0.666667  0.000000  0.000000  0.0\n",
       "31  0.666667  0.0  0.000000  0.000000  0.000000  0.333333  0.0\n",
       "32  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "33  0.000000  0.0  0.000000  0.666667  0.333333  0.000000  0.0\n",
       "34  0.333333  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "35  0.333333  0.0  0.000000  0.000000  0.333333  0.000000  0.0\n",
       "36  1.000000  0.0  0.000000  1.000000  0.000000  0.000000  0.0\n",
       "37  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "38  0.666667  0.0  0.000000  0.333333  0.666667  0.000000  0.0\n",
       "39  0.000000  0.0  0.000000  0.000000  0.333333  0.000000  0.0\n",
       "40  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "41  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "42  0.000000  0.0  1.000000  0.000000  0.000000  0.000000  0.0\n",
       "43  0.000000  0.0  0.000000  1.000000  0.666667  0.000000  0.0\n",
       "44  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "45  0.333333  0.0  0.000000  0.000000  0.000000  0.666667  0.0\n",
       "46  0.666667  0.0  0.000000  0.000000  0.000000  0.666667  0.0\n",
       "47  0.666667  0.0  0.000000  0.000000  0.333333  0.333333  0.0\n",
       "48  0.000000  0.0  0.000000  0.666667  1.000000  0.000000  0.0\n",
       "49  0.000000  0.0  0.000000  1.000000  0.000000  0.000000  0.0\n",
       "50  0.000000  0.0  0.000000  1.000000  1.000000  0.000000  0.0\n",
       "51  0.000000  0.0  0.000000  0.666667  1.000000  0.000000  0.0\n",
       "52  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
       "53  0.666667  0.0  0.000000  0.000000  0.000000  0.666667  0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_fs = X_valid_transform_f.iloc[:,feature_index]\n",
    "X_valid_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65f66621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>14</th>\n",
       "      <th>19</th>\n",
       "      <th>25</th>\n",
       "      <th>28</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          4         6         14        19        25        28        30\n",
       "0   0.333333  0.000000  0.000000  1.000000  0.666667  0.000000  0.000000\n",
       "1   0.333333  0.000000  0.000000  0.666667  0.000000  0.000000  0.000000\n",
       "2   0.000000  0.000000  0.000000  1.000000  0.666667  0.000000  0.000000\n",
       "3   0.000000  0.000000  0.000000  0.666667  0.666667  0.000000  0.000000\n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "5   0.000000  0.000000  0.666667  0.000000  0.000000  0.000000  0.000000\n",
       "6   0.333333  0.000000  0.000000  0.000000  0.000000  0.666667  0.000000\n",
       "7   0.666667  0.000000  0.000000  0.000000  0.666667  1.000000  0.000000\n",
       "8   0.666667  0.000000  0.000000  0.666667  1.000000  0.000000  0.000000\n",
       "9   0.333333  0.000000  0.000000  0.000000  0.666667  0.666667  0.000000\n",
       "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "11  0.000000  0.333333  0.000000  0.333333  0.000000  0.000000  0.333333\n",
       "12  0.666667  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "14  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000\n",
       "15  1.000000  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000\n",
       "16  0.000000  0.000000  0.000000  0.000000  0.000000  0.666667  0.000000\n",
       "17  0.000000  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000\n",
       "18  0.333333  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "19  0.000000  0.000000  0.000000  0.666667  0.000000  0.000000  0.000000\n",
       "20  0.333333  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "21  1.000000  0.000000  0.000000  0.666667  0.333333  0.000000  0.000000\n",
       "22  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000  1.000000\n",
       "23  0.000000  0.000000  0.000000  1.000000  0.666667  0.000000  0.000000\n",
       "24  0.000000  0.000000  0.000000  0.666667  0.666667  0.000000  0.000000\n",
       "25  0.666667  0.000000  0.666667  0.000000  0.000000  0.666667  0.000000\n",
       "26  0.000000  0.000000  0.000000  1.000000  1.000000  0.000000  0.000000\n",
       "27  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000\n",
       "28  0.000000  0.000000  0.000000  0.333333  0.666667  0.000000  0.000000\n",
       "29  0.000000  0.666667  0.000000  0.000000  0.000000  0.000000  0.666667\n",
       "30  1.000000  0.000000  0.000000  0.000000  0.000000  0.666667  0.000000\n",
       "31  0.000000  0.333333  0.666667  0.000000  0.000000  0.000000  0.000000\n",
       "32  0.000000  0.000000  0.333333  0.000000  0.000000  0.000000  0.000000\n",
       "33  0.000000  0.000000  0.000000  0.000000  0.000000  0.333333  0.000000\n",
       "34  0.000000  0.000000  0.000000  1.000000  0.666667  0.000000  0.000000\n",
       "35  0.333333  0.000000  0.000000  0.666667  0.666667  0.000000  0.000000\n",
       "36  0.666667  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "37  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "38  0.333333  0.000000  0.000000  0.666667  0.000000  0.000000  0.000000\n",
       "39  0.000000  0.000000  0.000000  0.333333  0.000000  0.000000  0.000000\n",
       "40  0.333333  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "41  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "42  0.000000  0.000000  0.666667  0.000000  0.000000  0.000000  0.000000\n",
       "43  0.000000  0.000000  0.000000  1.000000  1.000000  0.000000  0.000000\n",
       "44  0.666667  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "45  0.333333  0.000000  0.000000  0.666667  0.333333  0.000000  0.000000\n",
       "46  0.333333  0.000000  0.000000  0.000000  0.000000  0.666667  0.000000\n",
       "47  1.000000  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000\n",
       "48  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "49  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "50  0.000000  0.666667  0.000000  0.000000  0.000000  0.000000  0.666667\n",
       "51  0.666667  0.000000  0.000000  0.666667  0.000000  0.000000  0.000000\n",
       "52  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "53  0.000000  0.000000  0.000000  0.666667  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_fs = X_test_transform_f.iloc[:,feature_index]\n",
    "X_test_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dfc987",
   "metadata": {},
   "source": [
    "#### Data modeling\n",
    "Description: We will be building two predictive models to predict our target variable, which are K-Nearest Neighbours (KNN) and Neural Network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c07133",
   "metadata": {},
   "source": [
    "### 1. K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3539993",
   "metadata": {},
   "source": [
    "First, we train the neural network model with randomly initialized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2714f84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: \n",
      "\n",
      "Accuracy: 0.8703703703703703 \n",
      "\n",
      "Confusion matrix:\n",
      " [[16  1  0  0  0  0]\n",
      " [ 0 11  0  0  0  0]\n",
      " [ 0  1 11  3  0  0]\n",
      " [ 0  1  0  4  0  0]\n",
      " [ 0  1  0  0  4  0]\n",
      " [ 0  0  0  0  0  1]] \n",
      "\n",
      "Precision, recall, F1-score:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.94      0.97        17\n",
      "           2       0.73      1.00      0.85        11\n",
      "           3       1.00      0.73      0.85        15\n",
      "           4       0.57      0.80      0.67         5\n",
      "           5       1.00      0.80      0.89         5\n",
      "           6       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.87        54\n",
      "   macro avg       0.88      0.88      0.87        54\n",
      "weighted avg       0.91      0.87      0.88        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KNN= KNeighborsClassifier(n_neighbors=21)  \n",
    "KNN.fit(X_train_fs, y_train)\n",
    "predictions = KNN.predict(X_valid_fs)\n",
    "\n",
    "print('Validation set: \\n')\n",
    "print('Accuracy:', accuracy_score(y_valid, predictions), '\\n')\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_valid, predictions), '\\n')\n",
    "print('Precision, recall, F1-score:\\n', classification_report(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ccc48",
   "metadata": {},
   "source": [
    "Then, we find the best parameters of KNN by using GridSearchCV and compare the result with the previous result produced with random parameters to prove the improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c12052e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params={\n",
    "    \"n_neighbors\":range(1,30,2),\n",
    "    \"weights\":[\"uniform\", \"distance\"],\n",
    "    \"metric\":[\"euclidean\", \"manhattan\",\"minkowski\"],\n",
    "    \"algorithm\":[\"auto\",\"ball_tree\",\"kd_tree\",\"brute\"],\n",
    "    \"leaf_size\":range(1,50,5)\n",
    "}\n",
    "\n",
    "#grid search\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=KNN, param_grid=knn_params, n_jobs=-2, cv=cv, scoring=\"accuracy\", error_score=0)\n",
    "grid_results = grid_search.fit(X_train_fs, y_train)\n",
    "\n",
    "#best model\n",
    "final_model = KNN.set_params(**grid_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60937916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: \n",
      "\n",
      "Accuracy: 0.9259259259259259 \n",
      "\n",
      "Confusion matrix:\n",
      " [[17  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  0]\n",
      " [ 0  1 11  3  0  0]\n",
      " [ 0  0  0  5  0  0]\n",
      " [ 0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  1]] \n",
      "\n",
      "Precision, recall, F1-score:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        17\n",
      "           2       0.92      1.00      0.96        11\n",
      "           3       1.00      0.73      0.85        15\n",
      "           4       0.62      1.00      0.77         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.92      0.96      0.93        54\n",
      "weighted avg       0.95      0.93      0.93        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model.fit(X_train_fs, y_train)\n",
    "pred = final_model.predict(X_valid_fs)\n",
    "\n",
    "print('Validation set: \\n')\n",
    "print('Accuracy:', accuracy_score(y_valid, pred), '\\n')\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_valid, pred), '\\n')\n",
    "print('Precision, recall, F1-score:\\n', classification_report(y_valid, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e09db4",
   "metadata": {},
   "source": [
    "### 2. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39c244d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(units=100):\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(34,)),\n",
    "    tf.keras.layers.Dense(units, activation='relu', dtype='float64', kernel_regularizer='l1'),\n",
    "    tf.keras.layers.Dense(units, activation='relu', dtype='float64', kernel_regularizer='l1'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(6, activation='softmax', kernel_regularizer='l1')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca0f9a",
   "metadata": {},
   "source": [
    "First, we train the neural network model with randomly initialized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ba3fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 52ms/step - loss: 5.6836 - accuracy: 0.1888 - val_loss: 5.3606 - val_accuracy: 0.1667\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 4.9893 - accuracy: 0.4498 - val_loss: 5.1801 - val_accuracy: 0.3519\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 4.5454 - accuracy: 0.6426 - val_loss: 5.0396 - val_accuracy: 0.3889\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.2701 - accuracy: 0.7590 - val_loss: 4.9069 - val_accuracy: 0.6111\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 4.0431 - accuracy: 0.8514 - val_loss: 4.7763 - val_accuracy: 0.7222\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.8763 - accuracy: 0.8916 - val_loss: 4.6480 - val_accuracy: 0.7778\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.7486 - accuracy: 0.9116 - val_loss: 4.5250 - val_accuracy: 0.7963\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.6080 - accuracy: 0.9157 - val_loss: 4.4126 - val_accuracy: 0.8333\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3.4939 - accuracy: 0.9357 - val_loss: 4.3091 - val_accuracy: 0.8333\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 3.4026 - accuracy: 0.9357 - val_loss: 4.2048 - val_accuracy: 0.8519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21c5f5f13a0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomP_model = create_model(32) \n",
    "randomP_model.fit(X_train_transform, np.delete(to_categorical(y_train),0,1), validation_data=(X_valid_transform, np.delete(to_categorical(y_valid),0,1)), batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a6d54f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step\n",
      "Validation set: \n",
      "\n",
      "Accuracy: 0.8518518518518519 \n",
      "\n",
      "Confusion matrix:\n",
      " [[17  0  0  0  0  0]\n",
      " [ 0  7  0  4  0  0]\n",
      " [ 0  0 15  0  0  0]\n",
      " [ 1  2  0  2  0  0]\n",
      " [ 1  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  1]] \n",
      "\n",
      "Precision, recall, F1-score:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      1.00      0.94        17\n",
      "           2       0.78      0.64      0.70        11\n",
      "           3       1.00      1.00      1.00        15\n",
      "           4       0.33      0.40      0.36         5\n",
      "           5       1.00      0.80      0.89         5\n",
      "           6       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.83      0.81      0.82        54\n",
      "weighted avg       0.86      0.85      0.85        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = randomP_model.predict(X_valid_transform)\n",
    "y_pred = 1 + np.argmax(y_pred, axis=1)\n",
    "\n",
    "print('Validation set: \\n')\n",
    "print('Accuracy:',accuracy_score(y_valid, y_pred), '\\n')\n",
    "print('Confusion matrix:\\n',confusion_matrix(y_valid, y_pred), '\\n')\n",
    "print('Precision, recall, F1-score:\\n',classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1db3a",
   "metadata": {},
   "source": [
    "Next, we find the best parameters of Neural Network by using GridSearchCV and compare the result with the previous result produced with random parameters to prove the improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14ed0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'units': [64, 100, 128],\n",
    "    'batch_size': [32, 64, 100],\n",
    "    'epochs': [50, 100, 150]  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97299608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dbd97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ca8270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021C609110D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021C60905AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x0000021C602F7790&gt;,\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 100], &#x27;epochs&#x27;: [50, 100, 150],\n",
       "                         &#x27;units&#x27;: [64, 100, 128]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x0000021C602F7790&gt;,\n",
       "             param_grid={&#x27;batch_size&#x27;: [32, 64, 100], &#x27;epochs&#x27;: [50, 100, 150],\n",
       "                         &#x27;units&#x27;: [64, 100, 128]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x0000021C602F7790&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x0000021C602F7790&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x0000021C602F7790>,\n",
       "             param_grid={'batch_size': [32, 64, 100], 'epochs': [50, 100, 150],\n",
       "                         'units': [64, 100, 128]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train_transform,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "272e92b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'batch_size': 32, 'epochs': 100, 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a02469c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_c = np.delete(to_categorical(y_train),0,1)\n",
    "y_val_c = np.delete(to_categorical(y_valid),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81b62785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 45ms/step - loss: 9.4335 - accuracy: 0.3012 - val_loss: 9.0938 - val_accuracy: 0.2778\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.2406 - accuracy: 0.7028 - val_loss: 8.7016 - val_accuracy: 0.7963\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.6575 - accuracy: 0.8635 - val_loss: 8.3547 - val_accuracy: 0.8889\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.2646 - accuracy: 0.9197 - val_loss: 8.0399 - val_accuracy: 0.9259\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.9547 - accuracy: 0.9277 - val_loss: 7.7452 - val_accuracy: 0.9259\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.6587 - accuracy: 0.9478 - val_loss: 7.4638 - val_accuracy: 0.9259\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.3758 - accuracy: 0.9598 - val_loss: 7.1911 - val_accuracy: 0.9259\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.1141 - accuracy: 0.9679 - val_loss: 6.9236 - val_accuracy: 0.9259\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.8446 - accuracy: 0.9759 - val_loss: 6.6666 - val_accuracy: 0.9444\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.6011 - accuracy: 0.9839 - val_loss: 6.4166 - val_accuracy: 0.9444\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.3478 - accuracy: 0.9839 - val_loss: 6.1733 - val_accuracy: 0.9630\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.1067 - accuracy: 0.9839 - val_loss: 5.9358 - val_accuracy: 0.9630\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.8944 - accuracy: 0.9799 - val_loss: 5.7073 - val_accuracy: 0.9630\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.6670 - accuracy: 0.9759 - val_loss: 5.4831 - val_accuracy: 0.9630\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.4401 - accuracy: 0.9920 - val_loss: 5.2644 - val_accuracy: 0.9630\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.2279 - accuracy: 0.9960 - val_loss: 5.0502 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.0144 - accuracy: 0.9920 - val_loss: 4.8462 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.8197 - accuracy: 0.9920 - val_loss: 4.6540 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6311 - accuracy: 0.9960 - val_loss: 4.4723 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.4683 - accuracy: 0.9839 - val_loss: 4.3013 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3055 - accuracy: 0.9839 - val_loss: 4.1356 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.1309 - accuracy: 0.9920 - val_loss: 3.9771 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.9673 - accuracy: 0.9960 - val_loss: 3.8292 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.8177 - accuracy: 0.9920 - val_loss: 3.6864 - val_accuracy: 0.9815\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.6834 - accuracy: 0.9839 - val_loss: 3.5566 - val_accuracy: 0.9815\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.5645 - accuracy: 0.9920 - val_loss: 3.4365 - val_accuracy: 0.9630\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.4404 - accuracy: 0.9920 - val_loss: 3.3221 - val_accuracy: 0.9815\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.3359 - accuracy: 0.9920 - val_loss: 3.2125 - val_accuracy: 0.9630\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.2224 - accuracy: 0.9839 - val_loss: 3.1075 - val_accuracy: 0.9815\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.1291 - accuracy: 0.9920 - val_loss: 3.0077 - val_accuracy: 0.9815\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.0331 - accuracy: 0.9920 - val_loss: 2.9118 - val_accuracy: 0.9815\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.9557 - accuracy: 0.9920 - val_loss: 2.8237 - val_accuracy: 0.9815\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.8827 - accuracy: 0.9839 - val_loss: 2.7421 - val_accuracy: 0.9815\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.8062 - accuracy: 0.9799 - val_loss: 2.6595 - val_accuracy: 0.9815\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.7303 - accuracy: 0.9880 - val_loss: 2.5780 - val_accuracy: 0.9815\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.6618 - accuracy: 0.9960 - val_loss: 2.5038 - val_accuracy: 0.9815\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.6023 - accuracy: 0.9920 - val_loss: 2.4245 - val_accuracy: 0.9815\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.5452 - accuracy: 0.9920 - val_loss: 2.3568 - val_accuracy: 0.9815\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.4955 - accuracy: 0.9839 - val_loss: 2.2930 - val_accuracy: 0.9815\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.4414 - accuracy: 0.9839 - val_loss: 2.2267 - val_accuracy: 0.9815\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.4030 - accuracy: 0.9799 - val_loss: 2.1619 - val_accuracy: 0.9815\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.3529 - accuracy: 0.9759 - val_loss: 2.0974 - val_accuracy: 0.9815\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.3048 - accuracy: 0.9839 - val_loss: 2.0460 - val_accuracy: 0.9815\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2565 - accuracy: 0.9880 - val_loss: 1.9827 - val_accuracy: 0.9815\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2155 - accuracy: 0.9839 - val_loss: 1.9293 - val_accuracy: 0.9815\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1827 - accuracy: 0.9880 - val_loss: 1.8792 - val_accuracy: 0.9815\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1388 - accuracy: 0.9880 - val_loss: 1.8207 - val_accuracy: 0.9815\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1176 - accuracy: 0.9920 - val_loss: 1.7692 - val_accuracy: 0.9815\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0726 - accuracy: 0.9920 - val_loss: 1.7171 - val_accuracy: 0.9815\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0436 - accuracy: 0.9880 - val_loss: 1.6687 - val_accuracy: 0.9815\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0164 - accuracy: 0.9880 - val_loss: 1.6140 - val_accuracy: 0.9815\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.9846 - accuracy: 0.9920 - val_loss: 1.5635 - val_accuracy: 0.9815\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.9636 - accuracy: 0.9799 - val_loss: 1.5163 - val_accuracy: 0.9815\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.9398 - accuracy: 0.9920 - val_loss: 1.4737 - val_accuracy: 0.9815\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.9112 - accuracy: 0.9880 - val_loss: 1.4204 - val_accuracy: 0.9815\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.8873 - accuracy: 0.9880 - val_loss: 1.3647 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.8743 - accuracy: 0.9839 - val_loss: 1.3193 - val_accuracy: 0.9815\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.8482 - accuracy: 0.9880 - val_loss: 1.2751 - val_accuracy: 0.9815\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.8282 - accuracy: 0.9880 - val_loss: 1.2313 - val_accuracy: 0.9815\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.8119 - accuracy: 0.9880 - val_loss: 1.1898 - val_accuracy: 0.9815\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.8065 - accuracy: 0.9839 - val_loss: 1.1473 - val_accuracy: 0.9815\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7810 - accuracy: 0.9799 - val_loss: 1.1035 - val_accuracy: 0.9815\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7676 - accuracy: 0.9799 - val_loss: 1.0553 - val_accuracy: 0.9815\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7496 - accuracy: 0.9880 - val_loss: 1.0181 - val_accuracy: 0.9815\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7358 - accuracy: 0.9880 - val_loss: 0.9818 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7188 - accuracy: 0.9920 - val_loss: 0.9413 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7013 - accuracy: 0.9880 - val_loss: 0.9103 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.9839 - val_loss: 0.8863 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6666 - accuracy: 0.9880 - val_loss: 0.8581 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6778 - accuracy: 0.9960 - val_loss: 0.8364 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6522 - accuracy: 0.9920 - val_loss: 0.8125 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6381 - accuracy: 0.9920 - val_loss: 0.7854 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6237 - accuracy: 0.9880 - val_loss: 0.7629 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6231 - accuracy: 0.9880 - val_loss: 0.7368 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.6001 - accuracy: 0.9920 - val_loss: 0.7154 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5935 - accuracy: 0.9920 - val_loss: 0.6943 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5869 - accuracy: 0.9880 - val_loss: 0.6728 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5729 - accuracy: 0.9920 - val_loss: 0.6580 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5688 - accuracy: 0.9880 - val_loss: 0.6383 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5678 - accuracy: 0.9880 - val_loss: 0.6265 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5607 - accuracy: 0.9880 - val_loss: 0.6129 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5547 - accuracy: 0.9920 - val_loss: 0.5995 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5407 - accuracy: 0.9920 - val_loss: 0.5787 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5460 - accuracy: 0.9839 - val_loss: 0.5677 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5251 - accuracy: 0.9920 - val_loss: 0.5609 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5191 - accuracy: 0.9920 - val_loss: 0.5509 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5187 - accuracy: 0.9920 - val_loss: 0.5408 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5129 - accuracy: 0.9920 - val_loss: 0.5276 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5101 - accuracy: 0.9920 - val_loss: 0.5218 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5021 - accuracy: 0.9920 - val_loss: 0.5141 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4900 - accuracy: 0.9920 - val_loss: 0.5020 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4911 - accuracy: 0.9920 - val_loss: 0.4937 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5010 - accuracy: 0.9839 - val_loss: 0.4906 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4804 - accuracy: 0.9960 - val_loss: 0.4830 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4708 - accuracy: 0.9880 - val_loss: 0.4734 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4615 - accuracy: 1.0000 - val_loss: 0.4707 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4714 - accuracy: 0.9880 - val_loss: 0.4643 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4804 - accuracy: 0.9839 - val_loss: 0.4644 - val_accuracy: 0.9815\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4682 - accuracy: 0.9880 - val_loss: 0.4503 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4449 - accuracy: 1.0000 - val_loss: 0.4476 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21c5f0126d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = create_model(grid_search.best_params_['units']) \n",
    "best_model.fit(X_train_transform, y_train_c ,\n",
    "validation_data=(X_valid_transform, y_val_c),\n",
    "batch_size=grid_search.best_params_['batch_size'], epochs=grid_search.best_params_['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8cbbe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "Validation set: \n",
      "\n",
      "Accuracy: 1.0 \n",
      "\n",
      "Confusion matrix:\n",
      " [[17  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  0]\n",
      " [ 0  0 15  0  0  0]\n",
      " [ 0  0  0  5  0  0]\n",
      " [ 0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  1]] \n",
      "\n",
      "Precision, recall, F1-score:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        17\n",
      "           2       1.00      1.00      1.00        11\n",
      "           3       1.00      1.00      1.00        15\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00        54\n",
      "   macro avg       1.00      1.00      1.00        54\n",
      "weighted avg       1.00      1.00      1.00        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_valid_transform)\n",
    "y_pred = 1 + np.argmax(y_pred, axis=1)\n",
    "\n",
    "print('Validation set: \\n')\n",
    "print('Accuracy:',accuracy_score(y_valid, y_pred), '\\n')\n",
    "print('Confusion matrix:\\n',confusion_matrix(y_valid, y_pred), '\\n')\n",
    "print('Precision, recall, F1-score:\\n',classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d16fa1",
   "metadata": {},
   "source": [
    "#### Evaluate the models\n",
    "Perform a comparison between the predictive models. <br>\n",
    "Report the accuracy, recall, precision and F1-score measures as well as the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7d0a7",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0672c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9259259259259259 \n",
      "\n",
      "Confusion matrix:\n",
      " [[19  3  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  1 10  0  0  0]\n",
      " [ 0  0  0  6  0  0]\n",
      " [ 0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  4]] \n",
      "\n",
      "Precision, recall, F1-score:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.86      0.93        22\n",
      "           2       0.56      1.00      0.71         5\n",
      "           3       1.00      0.91      0.95        11\n",
      "           4       1.00      1.00      1.00         6\n",
      "           5       1.00      1.00      1.00         6\n",
      "           6       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.93      0.96      0.93        54\n",
      "weighted avg       0.96      0.93      0.93        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict1 = final_model.predict(X_test_fs)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test,y_predict1), '\\n')\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test,y_predict1), '\\n')\n",
    "print('Precision, recall, F1-score:\\n', classification_report(y_test,y_predict1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f32dd4",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6ab5721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test_transform)\n",
    "y_pred = np.argmax(y_pred, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8af3f8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0 \n",
      "\n",
      "Confusion matrix:\n",
      " [[22  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  0 11  0  0  0]\n",
      " [ 0  0  0  6  0  0]\n",
      " [ 0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  4]] \n",
      "\n",
      "Precision, recall, F1-score:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        22\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00        11\n",
      "           4       1.00      1.00      1.00         6\n",
      "           5       1.00      1.00      1.00         6\n",
      "           6       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00        54\n",
      "   macro avg       1.00      1.00      1.00        54\n",
      "weighted avg       1.00      1.00      1.00        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',accuracy_score(y_test, y_pred), '\\n')\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred), '\\n')\n",
    "print('Precision, recall, F1-score:\\n',classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846adffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
